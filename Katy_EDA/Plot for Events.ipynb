{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"calendar.csv\")\n",
    "sales = pd.read_csv(\"sales_train_validation.csv\")\n",
    "sell_price = pd.read_csv(\"sell_prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_use = sales.iloc[:,:-28]\n",
    "valid = pd.concat([sales.iloc[:,:6], sales.iloc[:,-28:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by category & state\n",
    "for_use[(for_use['cat_id']=='HOBBIES')&(for_use['state_id']=='CA')].shape\n",
    "sales_g1 = for_use[(for_use['cat_id']=='HOBBIES')&(for_use['state_id']=='CA')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "sales_g1_melt = pd.melt(sales_g1, id_vars=sales_g1.columns[:6], value_vars=sales_g1.columns[6:])\n",
    "sales_g1_melt.columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all dataset\n",
    "sales_g1_melt_merge = pd.merge(sales_g1_melt, calendar)\n",
    "sales_g1_melt_merge = pd.merge(sales_g1_melt_merge, sell_price, how = 'left')\n",
    "sales_g1_melt_merge = sales_g1_melt_merge.sort_values(['id', 'date']).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some useless columns\n",
    "df = sales_g1_melt_merge[['item_id', 'dept_id', 'store_id', 'd', 'sales', 'wm_yr_wk', 'wday', 'month', 'year',\n",
    "                          'event_name_1', 'event_name_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['event_name_1'] = (df['event_name_1'].notnull()).astype('int')\n",
    "# df['event_name_2'] = (df['event_name_2'].notnull()).astype('int')\n",
    "df['wm_yr_wk'] = df['wm_yr_wk'].apply(lambda x: int(str(x)[-2:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_name_1_Name'] = df['event_name_1']\n",
    "df['event_name_2_Name'] = df['event_name_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing value: NaN = 0\n",
    "# astype('str') since LabelEncoder needs consistent datatype\n",
    "df[\"event_name_1\"] = df[\"event_name_1\"].fillna(0).astype('str')\n",
    "df[\"event_name_2\"] = df[\"event_name_2\"].fillna(0).astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the events\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df[\"event_name_1\"] = labelencoder.fit_transform(df[\"event_name_1\"])\n",
    "df[\"event_name_2\"] = labelencoder.fit_transform(df[\"event_name_2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accumulated (2011-2016 in total) Sales based on Event 1-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_event1 = df.loc[df['event_name_1'] != 0]\n",
    "df_without_event1 = df.loc[df['event_name_1'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_event1_Name = pd.DataFrame(df_with_event1['sales'].groupby(df_with_event1['event_name_1_Name']).sum()).reset_index()\n",
    "with_event1_number = pd.DataFrame(df_with_event1['sales'].groupby(df_with_event1['event_name_1']).sum()).reset_index()\n",
    "event_1 = pd.merge(with_event1_number, with_event1_Name)\n",
    "event_1 = event_1[['event_name_1', 'event_name_1_Name', 'sales']]\n",
    "# without_event1 = df_without_event1['sales'].groupby(df_without_event1['year']).sum()\n",
    "print(event_1)\n",
    "plt.plot(with_event1_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sales of Event 1-30 based in different year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_event_yearly = df_with_event1.groupby(['event_name_1', 'year']).sum()\n",
    "for i in range(1, 31):\n",
    "    x = each_event_yearly['sales'].loc[i]\n",
    "    plt.plot(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unused\n",
    "\n",
    "each_event_yearly_2 = df_with_event1.groupby(['event_name_1', 'year']).sum().reset_index()\n",
    "each_event_yearly_2\n",
    "\n",
    "x = each_event_yearly_2.groupby('event_name_1')\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weigh the \"Event\" Feature base on Total Sales Volume in 2011-2016\n",
    "* The higher the event boosts sales, the more the event weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_1 = event_1.sort_values(by = ['sales'])\n",
    "event_1['event_weighted'] = range(1, len(event_1)+1)\n",
    "# event_1\n",
    "event_1_dict = event_1.set_index('event_name_1_Name').T.to_dict('int')\n",
    "event_1_dict['event_weighted']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Reference for updating column with dictionary values: https://stackoverflow.com/questions/20250771/remap-values-in-pandas-column-with-a-dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('event_name_1_Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_name_1'].update(pd.Series(event_1_dict['event_weighted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()\n",
    "df['event_name_1'] = df['event_name_1'].astype('int')\n",
    "df['event_name_2'] = df['event_name_2'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop event name columns\n",
    "df.drop(columns = ['event_name_1_Name', 'event_name_2_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train : d_1 ~ d_1913\n",
    "train = df.loc[df['d'].isin(list(df['d'].unique()[:-56]))]\n",
    "# train : d_1914 ~ d_1969\n",
    "test = df.loc[df['d'].isin(list(df['d'].unique()[-56:]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the data with no price info\n",
    "train = train[train['sell_price'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train['sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_CA', 'sell_price']]\n",
    "y_train = train['sales'].fillna(0).astype('int')\n",
    "X_test = test[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_CA', 'sell_price']]\n",
    "y_test = test['sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start to Run from Here... :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(\"calendar.csv\")\n",
    "sales = pd.read_csv(\"sales_train_validation.csv\")\n",
    "sell_price = pd.read_csv(\"sell_prices.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in the columns to be predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_fill_date = sales.copy()\n",
    "date_list = []\n",
    "for i in range(1914, 1970):\n",
    "    date_list.append('d_'+str(i))\n",
    "fill_date = pd.DataFrame(columns = date_list)\n",
    "sales_fill_date = pd.concat([sales_fill_date, fill_date], axis = 1)\n",
    "# print(sales_fill_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d                                id  d_1314  d_1315  d_1316  d_1317  d_1318  \\\n",
      "0     HOBBIES_1_001_CA_1_validation     1.0     1.0     1.0     1.0     1.0   \n",
      "1     HOBBIES_1_001_CA_2_validation     1.0     1.0     1.0     1.0     1.0   \n",
      "2     HOBBIES_1_001_CA_3_validation     1.0     1.0     1.0     1.0     1.0   \n",
      "3     HOBBIES_1_001_CA_4_validation     1.0     1.0     1.0     1.0     1.0   \n",
      "4     HOBBIES_1_002_CA_1_validation     0.0     0.0     0.0     0.0     0.0   \n",
      "...                             ...     ...     ...     ...     ...     ...   \n",
      "4306    FOODS_3_826_WI_2_validation     2.0     2.0     2.0     3.0     2.0   \n",
      "4307    FOODS_3_826_WI_3_validation     2.0     2.0     2.0     3.0     2.0   \n",
      "4308    FOODS_3_827_WI_1_validation     4.0     3.0     4.0     5.0     4.0   \n",
      "4309    FOODS_3_827_WI_2_validation    17.0     9.0    17.0    21.0    12.0   \n",
      "4310    FOODS_3_827_WI_3_validation     4.0     3.0     4.0     5.0     4.0   \n",
      "\n",
      "d     d_1319  d_1320  d_1338  d_1339  ...  d_950  d_951  d_952  d_953  d_954  \\\n",
      "0        1.0     1.0     1.0     1.0  ...    1.0    1.0    1.0    1.0    1.0   \n",
      "1        1.0     1.0     1.0     1.0  ...    1.0    1.0    1.0    1.0    1.0   \n",
      "2        1.0     1.0     1.0     1.0  ...    1.0    1.0    1.0    1.0    1.0   \n",
      "3        1.0     1.0     1.0     1.0  ...    1.0    1.0    1.0    1.0    1.0   \n",
      "4        0.0     0.0     0.0     0.0  ...    0.0    0.0    0.0    0.0    0.0   \n",
      "...      ...     ...     ...     ...  ...    ...    ...    ...    ...    ...   \n",
      "4306     2.0     2.0     2.0     2.0  ...    3.0    3.0    3.0    4.0    4.0   \n",
      "4307     2.0     2.0     2.0     2.0  ...    3.0    3.0    3.0    4.0    4.0   \n",
      "4308     4.0     4.0     4.0     4.0  ...    9.0   17.0   17.0   12.0   21.0   \n",
      "4309    17.0    17.0    12.0    12.0  ...    9.0   17.0   17.0   12.0   21.0   \n",
      "4310     4.0     4.0     4.0     4.0  ...    9.0   17.0   17.0   12.0   21.0   \n",
      "\n",
      "d     d_955  d_973  d_974  d_975  d_976  \n",
      "0       1.0    1.0    1.0    1.0    1.0  \n",
      "1       1.0    1.0    1.0    1.0    1.0  \n",
      "2       1.0    1.0    1.0    1.0    1.0  \n",
      "3       1.0    1.0    1.0    1.0    1.0  \n",
      "4       0.0    0.0    0.0    0.0    0.0  \n",
      "...     ...    ...    ...    ...    ...  \n",
      "4306    3.0    3.0    5.0    4.0    4.0  \n",
      "4307    3.0    3.0    5.0    4.0    4.0  \n",
      "4308   17.0   10.0   12.0   12.0    9.0  \n",
      "4309   17.0   10.0   12.0   12.0    9.0  \n",
      "4310   17.0   10.0   12.0   12.0    9.0  \n",
      "\n",
      "[30490 rows x 57 columns]\n"
     ]
    }
   ],
   "source": [
    "final_all = pd.DataFrame()\n",
    "for cat in sales['cat_id'].unique():\n",
    "    for state in sales['state_id'].unique():\n",
    "        sales_group = sales_fill_date[(sales_fill_date['cat_id']==cat)&(sales_fill_date['state_id']==state)]\n",
    "        sales_group_melt = pd.melt(sales_group, id_vars=sales_group.columns[:6], value_vars=sales_group.columns[6:])\n",
    "        sales_group_melt.columns = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd', 'sales']\n",
    "        sales_group_melt_merge = pd.merge(sales_group_melt, calendar)\n",
    "        sales_group_melt_merge = pd.merge(sales_group_melt_merge, sell_price, how = 'left')\n",
    "        sales_group_melt_merge = sales_group_melt_merge.sort_values(['id', 'date']).reset_index(drop = True)\n",
    "        if state == 'CA':\n",
    "            df = sales_group_melt_merge[['id', 'item_id', 'dept_id', 'store_id', 'd', 'sales', 'wm_yr_wk', 'wday', 'month', 'year',\n",
    "                                      'event_name_1', 'event_name_2', 'snap_CA', 'sell_price']].copy()\n",
    "        elif state == 'TX':\n",
    "            df = sales_group_melt_merge[['id', 'item_id', 'dept_id', 'store_id', 'd', 'sales', 'wm_yr_wk', 'wday', 'month', 'year',\n",
    "                                      'event_name_1', 'event_name_2', 'snap_TX', 'sell_price']].copy()\n",
    "        else:\n",
    "            df = sales_group_melt_merge[['id', 'item_id', 'dept_id', 'store_id', 'd', 'sales', 'wm_yr_wk', 'wday', 'month', 'year',\n",
    "                                         'event_name_1', 'event_name_2', 'snap_WI', 'sell_price']].copy()\n",
    "        # df['event_name_1'] = (df['event_name_1'].notnull()).astype('int')\n",
    "        # df['event_name_2'] = (df['event_name_2'].notnull()).astype('int')\n",
    "        # Extract the last two digits for week info\n",
    "        df['wm_yr_wk'] = df['wm_yr_wk'].apply(lambda x: int(str(x)[-2:]))\n",
    "        \n",
    "        \n",
    "        ### Encoding Events & Add Weight\n",
    "        df['event_name_1_Name'] = df['event_name_1']\n",
    "        df['event_name_2_Name'] = df['event_name_2']\n",
    "        \n",
    "        df[\"event_name_1\"] = df[\"event_name_1\"].fillna(0).astype('str')\n",
    "        df[\"event_name_2\"] = df[\"event_name_2\"].fillna(0).astype('str')\n",
    "        \n",
    "        labelencoder = LabelEncoder()\n",
    "        df[\"event_name_1\"] = labelencoder.fit_transform(df[\"event_name_1\"])\n",
    "        df[\"event_name_2\"] = labelencoder.fit_transform(df[\"event_name_2\"])\n",
    "        \n",
    "        df_with_event1 = df.loc[df['event_name_1'] != 0]\n",
    "        df_without_event1 = df.loc[df['event_name_1'] == 0]\n",
    "        \n",
    "        with_event1_Name = pd.DataFrame(df_with_event1['sales'].groupby(df_with_event1['event_name_1_Name']).sum()).reset_index()\n",
    "        with_event1_number = pd.DataFrame(df_with_event1['sales'].groupby(df_with_event1['event_name_1']).sum()).reset_index()\n",
    "        event_1 = pd.merge(with_event1_number, with_event1_Name)\n",
    "        event_1 = event_1[['event_name_1', 'event_name_1_Name', 'sales']]\n",
    "        \n",
    "        event_1 = event_1.sort_values(by = ['sales'])\n",
    "        event_1['event_weighted'] = range(1, len(event_1)+1)\n",
    "        event_1_dict = event_1.set_index('event_name_1_Name').T.to_dict('int')\n",
    "        \n",
    "        df = df.set_index('event_name_1_Name')\n",
    "        df['event_name_1'].update(pd.Series(event_1_dict['event_weighted']))\n",
    "        df = df.reset_index()\n",
    "        df['event_name_1'] = df['event_name_1'].astype('int')\n",
    "        df['event_name_2'] = df['event_name_2'].astype('int')\n",
    "        ###\n",
    "        \n",
    "        train = df.loc[df['d'].isin(list(df['d'].unique()[:-56]))]\n",
    "        test = df.loc[df['d'].isin(list(df['d'].unique()[-56:]))]\n",
    "        \n",
    "        train = train[train['sell_price'].notna()]\n",
    "        if state == 'CA':\n",
    "            X_train = train[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_CA', 'sell_price']]\n",
    "            X_test = test[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_CA', 'sell_price']]\n",
    "        elif state == 'TX':\n",
    "            X_train = train[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_TX', 'sell_price']]\n",
    "            X_test = test[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_TX', 'sell_price']]\n",
    "        else:\n",
    "            X_train = train[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_WI', 'sell_price']]\n",
    "            X_test = test[['wm_yr_wk', 'wday', 'month', 'year', 'event_name_1', 'event_name_2', 'snap_WI', 'sell_price']]\n",
    "        y_train = train['sales'].fillna(0).astype('int')    # Add fillna(0)\n",
    "        y_test = test['sales']\n",
    "\n",
    "        # Modeling\n",
    "        lgbm = lgb.LGBMRegressor()\n",
    "        lgbm.fit(X_train, y_train)\n",
    "        lgbm_pred = lgbm.predict(X_test)\n",
    "        lgbm_pred = np.round(lgbm_pred, 0)\n",
    "        final_pred = pd.concat([test[['id', 'd']].reset_index(drop=True), pd.DataFrame(lgbm_pred)], axis=1)\n",
    "        final_pred.columns = ['id', 'd', 'values']\n",
    "        final_pred = final_pred.pivot(index='id', columns='d', values='values').reset_index()\n",
    "        \n",
    "        final_all = pd.concat([final_all, final_pred], axis = 0)\n",
    "        \n",
    "print(final_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final submission processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 id    F1    F2    F3    F4    F5    F6    F7  \\\n",
      "0     HOBBIES_1_001_CA_1_validation   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
      "1     HOBBIES_1_001_CA_2_validation   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
      "2     HOBBIES_1_001_CA_3_validation   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
      "3     HOBBIES_1_001_CA_4_validation   1.0   1.0   1.0   1.0   1.0   1.0   1.0   \n",
      "4     HOBBIES_1_002_CA_1_validation   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "...                             ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "4306    FOODS_3_826_WI_2_evaluation  19.0  19.0  10.0  10.0  11.0  11.0  20.0   \n",
      "4307    FOODS_3_826_WI_3_evaluation  19.0  19.0  10.0  10.0  11.0  11.0  20.0   \n",
      "4308    FOODS_3_827_WI_1_evaluation  19.0  19.0  10.0  10.0  11.0  11.0  20.0   \n",
      "4309    FOODS_3_827_WI_2_evaluation  19.0  19.0  10.0  10.0  11.0  11.0  20.0   \n",
      "4310    FOODS_3_827_WI_3_evaluation  19.0  19.0  10.0  10.0  11.0  11.0  20.0   \n",
      "\n",
      "        F8    F9  ...  F19   F20   F21   F22   F23   F24   F25   F26   F27  \\\n",
      "0      1.0   1.0  ...  1.0   1.0   1.0   1.0   1.0   2.0   1.0   1.0   1.0   \n",
      "1      1.0   1.0  ...  1.0   1.0   1.0   1.0   1.0   2.0   1.0   1.0   1.0   \n",
      "2      1.0   1.0  ...  1.0   1.0   1.0   1.0   1.0   2.0   1.0   1.0   1.0   \n",
      "3      1.0   1.0  ...  1.0   1.0   1.0   1.0   1.0   2.0   1.0   1.0   1.0   \n",
      "4      0.0   0.0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "...    ...   ...  ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
      "4306  11.0  20.0  ...  3.0   3.0   3.0   4.0   4.0   3.0   3.0   5.0   4.0   \n",
      "4307  11.0  20.0  ...  3.0   3.0   3.0   4.0   4.0   3.0   3.0   5.0   4.0   \n",
      "4308  11.0  20.0  ...  9.0  17.0  17.0  12.0  21.0  17.0  10.0  12.0  12.0   \n",
      "4309  11.0  20.0  ...  9.0  17.0  17.0  12.0  21.0  17.0  10.0  12.0  12.0   \n",
      "4310  11.0  20.0  ...  9.0  17.0  17.0  12.0  21.0  17.0  10.0  12.0  12.0   \n",
      "\n",
      "      F28  \n",
      "0     1.0  \n",
      "1     1.0  \n",
      "2     1.0  \n",
      "3     1.0  \n",
      "4     0.0  \n",
      "...   ...  \n",
      "4306  4.0  \n",
      "4307  4.0  \n",
      "4308  9.0  \n",
      "4309  9.0  \n",
      "4310  9.0  \n",
      "\n",
      "[60980 rows x 29 columns]\n"
     ]
    }
   ],
   "source": [
    "# Validation : d_1914 ~ d_1941\n",
    "final_val = final_all.iloc[:,:29]\n",
    "# Evaluation : d_1942 ~ d_1969\n",
    "final_eval = pd.concat([final_all.iloc[:,0], final_all.iloc[:,29:]], axis = 1)\n",
    "final_eval['id'] = final_eval['id'].str.replace('validation', 'evaluation')\n",
    "\n",
    "submission_colnames = ['id']\n",
    "for i in range(1,29):\n",
    "    submission_colnames.append('F'+str(i))\n",
    "\n",
    "final_val.columns = submission_colnames\n",
    "final_eval.columns = submission_colnames\n",
    "\n",
    "submission = pd.concat([final_val, final_eval], axis = 0)\n",
    "# submission.to_csv(r'C:\\Users\\Katy\\Desktop\\Kaggle\\submission.csv', index = False)\n",
    "print(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
